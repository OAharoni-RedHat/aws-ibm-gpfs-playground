---
- name: Playbook to set up the Openshift Cluster
  hosts: localhost
  gather_facts: false
  become: false
  vars_files:
    # Use this to override stuff that won't be committed to git
    - ./overrides.yml
  tasks:
    - name: Create working folder
      tags:
        - 1_ocp_install
      ansible.builtin.file:
        path: "{{ ocpfolder }}"
        state: directory
        recurse: true

    - name: Does cluster metadata.json exist
      ansible.builtin.stat:
        path: "{{ ocpfolder }}/metadata.json"
      register: metadata_json_file

    - name: Template OCP install file
      tags:
        - 1_ocp_install
      ansible.builtin.template:
        src: ../templates/full-cluster-install-config.j2.yaml
        dest: "{{ ocpfolder }}/install-config.yaml"
      when: not metadata_json_file.stat.exists

    - name: Set kubeadmin password fact
      tags:
        - 1_ocp_install
      ansible.builtin.shell: |
        python -c 'import bcrypt; print(bcrypt.hashpw(b"{{ kubeadmin_pass }}", bcrypt.gensalt(rounds=10)).decode())' | base64 -w0
      register:
        hashed_password_out

    - name: Set hashed password fact
      tags:
        - 1_ocp_install
      ansible.builtin.set_fact:
        hashed_password: "{{ hashed_password_out.stdout }}"

    - name: Install ocp cluster
      tags:
        - 1_ocp_install
      ansible.builtin.shell: |
        set -ex
        id
        {{ basefolder }}/{{ ocp_version }}/openshift-install create cluster --dir=. &> /tmp/oc-{{ ocp_version }}-{{ ocp_cluster_name }}.log
      args:
        chdir: "{{ ocpfolder }}"
      when: not metadata_json_file.stat.exists

    - name: Does cluster kubeconfig exist
      ansible.builtin.stat:
        path: "{{ ocpfolder }}/auth/kubeconfig"
      register: kubeconfig_file

    - name: Fail here there is no kubeconfig file
      fail: msg="The openshift cluster kubeconfig file is missing, please check {{ ocpfolder }}.openshift_install.log"
      when: not kubeconfig_file.stat.exists

    - name: Set kubeadmin password
      tags:
        - 1_ocp_install
      ansible.builtin.shell: |
        set -e
        chmod 0600 ./auth/kubeconfig
        export KUBECONFIG=./auth/kubeconfig
        {{ oc_bin }} patch secret -n kube-system kubeadmin --type json -p '[{"op": "replace", "path": "/data/kubeadmin", "value": "'{{ hashed_password }}'"}]'
      args:
        chdir: "{{ ocpfolder }}"

    - name: Template mco file
      tags:
        - 1_ocp_install
      ansible.builtin.template:
        src: ../templates/mco.yaml
        dest: "{{ gpfsfolder }}/mco.yaml"

    # FIXME:
    # We apply the MCO template right away because that will cause the nodes to
    # reboot and only later we add the EBS shared volume
    # This is because GPFS refuses to read the /dev/disk/by-id/nvme-AMAZON..volid
    # symlink. This way we can just use /dev/nvme1n1 across the nodes
    - name: Apply MCO template
      tags:
        - 1_ocp_install
      ansible.builtin.shell: |
        set -ex
        export KUBECONFIG=./auth/kubeconfig
        {{ oc_bin }} apply -f {{ gpfsfolder }}/mco.yaml
      args:
        chdir: "{{ ocpfolder }}"

    # FIXME(bandini): Sleep here for a bit so we're sure it is started
    # later we should wait for the mcp's to go in updating status and then
    # wait for the finished condition
    - name: Wait a bit for MCP to start
      tags:
        - 1_ocp_install
      ansible.builtin.pause:
        minutes: 1

    - name: Speed up MCO
      tags:
        - 1_ocp_install
      ansible.builtin.shell: |
        set -ex
        export KUBECONFIG=./auth/kubeconfig
        oc patch mcp worker --type merge --patch '{"spec": {"maxUnavailable": 2}}'
      args:
        chdir: "{{ ocpfolder }}"

    - name: Wait for MCP to settle
      tags:
        - 1_ocp_install
      ansible.builtin.shell: |
        set -ex
        export KUBECONFIG=./auth/kubeconfig
        if [ $({{ oc_bin }} get mcp/master -o jsonpath='{.status.readyMachineCount}') != $({{ oc_bin }} get mcp/master -o jsonpath='{.status.machineCount}') ]; then
          exit 1
        fi
        if [ $({{ oc_bin }} get mcp/worker -o jsonpath='{.status.readyMachineCount}') != $({{ oc_bin }} get mcp/worker -o jsonpath='{.status.machineCount}') ]; then
          exit 1
        fi
      args:
        chdir: "{{ ocpfolder }}"
      retries: 30
      delay: 90
      register: mcp_ready
      until: mcp_ready is not failed

    # See https://www.ibm.com/docs/en/scalecontainernative/5.2.2?topic=aws-red-hat-openshift-configuration#authorize-rosa-worker-security-group-to-allow-ibm-storage-scale-container-native-ports
    - name: Gather security group info for workers
      tags:
        - 2_aws
      amazon.aws.ec2_security_group_info:
        region: "{{ ocp_region }}"
        filters:
          "tag:sigs.k8s.io/cluster-api-provider-aws/role": "node"
      register: sg_info

    - name: Debug sg_info
      tags:
        - 2_aws
      ansible.builtin.debug:
        msg: "{{ sg_info }}"

    - name: Set sg id
      tags:
        - 2_aws
      ansible.builtin.set_fact:
        sg_worker_id: "{{ sg_info.security_groups[0].group_id }}"

    # FIXME(bandini): use aws module here
    - name: Open up default security groups so gpfs can work in AWS
      tags:
        - 2_aws
      ansible.builtin.shell: |
        aws ec2 --region {{ ocp_region }} authorize-security-group-ingress --group-id {{ sg_worker_id }} --protocol tcp --port 12345 --source-group {{ sg_worker_id }}
        aws ec2 --region {{ ocp_region }} authorize-security-group-ingress --group-id {{ sg_worker_id }} --protocol tcp --port 1191 --source-group {{ sg_worker_id }}
        aws ec2 --region {{ ocp_region }} authorize-security-group-ingress --group-id {{ sg_worker_id }} --protocol tcp --port 60000-61000 --source-group {{ sg_worker_id }}

    # FIXME(bandini): this will need to be more robust
    - name: Find OpenShift EC2 Instances
      tags:
        - 3_ebs
      amazon.aws.ec2_instance_info:
        region: "{{ ocp_region }}"
        filters:
          "tag:Name": "{{ ocp_cluster_name }}*worker*"
          "instance-state-name": "running"
      register: ec2_workers

    - name: Set EC2 workers instance IDs
      tags:
        - 3_ebs
      ansible.builtin.set_fact:
        worker_ec2_ids: "{{ ec2_workers.instances | map(attribute='instance_id') | list }}"

    - name: Create EBS io2 volume
      tags:
        - 3_ebs
      amazon.aws.ec2_vol:
        region: "{{ ocp_region }}"
        availability_zone: "{{ ocp_az }}"
        volume_size: "{{ ebs_volume_size }}"
        volume_type: "{{ ebs_volume_type }}"
        multi_attach: yes
        iops: "{{ ebs_iops }}"
        tags:
          Name: "{{ gpfs_volume_name }}"
      register: ebs_volume

    - name: Attach EBS volume to workers
      tags:
        - 3_ebs
      amazon.aws.ec2_vol:
        region: "{{ ocp_region }}"
        instance: "{{ item }}"
        id: "{{ ebs_volume.volume_id }}"
        device_name: "{{ ebs_device_name }}"
      loop: "{{ worker_ec2_ids }}"
      when: ebs_volume.volume_id is defined

    - name: Install ibm spectrum manifest
      tags:
        - 4_gpfs
      ansible.builtin.shell: |
        set -e
        export KUBECONFIG=./auth/kubeconfig
        {{ oc_bin }} apply -f https://raw.githubusercontent.com/IBM/ibm-spectrum-scale-container-native/{{ gpfs_version }}/generated/scale/install.yaml
      args:
        chdir: "{{ ocpfolder }}"

    - name: Create gpfs folder
      tags:
        - 4_gpfs
      ansible.builtin.file:
        path: "{{ gpfsfolder }}"
        state: directory
        recurse: true

    - name: template ocp mirror
      tags:
        - 4_gpfs
      ansible.builtin.template:
        src: ../templates/imagedigestmirror.yaml
        dest: "{{ gpfsfolder }}/imagedigestmirror.yaml"

    - name: Apply mirror template
      tags:
        - 4_gpfs
      ansible.builtin.shell: |
        set -e
        export KUBECONFIG=./auth/kubeconfig
        {{ oc_bin }} apply -f "{{ gpfsfolder }}/imagedigestmirror.yaml"
      args:
        chdir: "{{ ocpfolder }}"

    - name: Update global pull secret
      tags:
        - 4_gpfs
      ansible.builtin.shell: |
        set -ex
        export KUBECONFIG=./auth/kubeconfig
        NEW_PS=$({{ oc_bin }} -n openshift-config get secret pull-secret -o jsonpath='{.data.\.dockerconfigjson}' | base64 -d  | jq -c '.auths += {"quay.io/rhsysdeseng":{"auth":"{{ ibmpullsecret }}", "email":""}}' | base64 -w 0)
        {{ oc_bin }} -n openshift-config patch secret pull-secret -p "{\"data\":{\".dockerconfigjson\":\"$NEW_PS\"}}"
      args:
        chdir: "{{ ocpfolder }}"

    - name: Create ibm secret
      tags:
        - 4_gpfs
      ansible.builtin.shell: |
        set -ex
        export KUBECONFIG=./auth/kubeconfig
        cat <<EOF > {{ gpfsfolder }}/ibmscaleps.json
        {"auths":{"quay.io/rhsysdeseng":{"auth":"{{ ibmpullsecret }}","email":""}}}
        EOF
        for namespace in ibm-spectrum-scale ibm-spectrum-scale-operator ibm-spectrum-scale-dns ibm-spectrum-scale-csi; do
          {{ oc_bin }} create secret docker-registry ibm-entitlement-key -n ${namespace} --from-file=.dockerconfigjson={{ gpfsfolder }}/ibmscaleps.json
        done
      args:
        chdir: "{{ ocpfolder }}"

    - name: Template cluster file
      tags:
        - 4_gpfs
      ansible.builtin.template:
        src: ../templates/cluster.yaml
        dest: "{{ gpfsfolder }}/cluster.yaml"

    - name: Label the workers
      tags:
        - 5_gpfs
      ansible.builtin.shell: |
        set -ex
        export KUBECONFIG=./auth/kubeconfig
        for node in $({{ oc_bin }} get nodes -l node-role.kubernetes.io/worker -o name)
        do
          {{ oc_bin }} label ${node} scale.spectrum.ibm.com/role=storage
          {{ oc_bin }} label ${node} scale.spectrum.ibm.com/daemon-selector=""
        done
      args:
        chdir: "{{ ocpfolder }}"

    - name: Apply the cluster template
      tags:
        - 5_gpfs
      ansible.builtin.shell: |
        set -ex
        export KUBECONFIG=./auth/kubeconfig
        {{ oc_bin }} apply -f "{{ gpfsfolder }}/cluster.yaml"
      args:
        chdir: "{{ ocpfolder }}"
      retries: 10
      delay: 30
      register: gpfs_cluster_ready
      until: gpfs_cluster_ready is not failed

    - name: Get the Volume ID by Tag Name again
      tags:
        - 6_gpfs
      amazon.aws.ec2_vol_info:
        region: "{{ ocp_region }}"
        filters:
          "tag:Name": "{{ gpfs_volume_name }}"
      register: volume_info

    - name: Fail if there is not exactly one ebs volume
      tags:
        - 6_gpfs
      ansible.builtin.fail:
        msg: "There must be only one ebs volumes called {{ gpfs_volume_name }}: {{ volume_info }}"
      when: volume_info.volumes | length != 1

    - name: Set volumeid fact
      tags:
        - 6_gpfs
      ansible.builtin.set_fact:
        ebs_volid: "{{ volume_info.volumes[0].id }}"

    - name: Get worker nodes names
      tags:
        - 6_gpfs
      ansible.builtin.shell: |
        export KUBECONFIG=./auth/kubeconfig
        {{ oc_bin }} get nodes -l node-role.kubernetes.io/worker -o name | cut -f2 -d/
      args:
        chdir: "{{ ocpfolder }}"
      register: worker_nodes_output

    - name: Set worker nodes names fact
      tags:
        - 6_gpfs
      ansible.builtin.set_fact:
        worker_nodes: "{{ worker_nodes_output.stdout_lines }}"

    - name: Template the localdisk
      tags:
        - 6_gpfs
      ansible.builtin.template:
        src: ../templates/localdisk.yaml
        dest: "{{ gpfsfolder }}/localdisk.yaml"

    - name: Apply the localdisk
      tags:
        - 6_gpfs
      ansible.builtin.shell: |
        set -ex
        export KUBECONFIG=./auth/kubeconfig
        {{ oc_bin }} apply -f "{{ gpfsfolder }}/localdisk.yaml"
      args:
        chdir: "{{ ocpfolder }}"

    - name: Template the filesystem
      tags:
        - 7_gpfs
      ansible.builtin.template:
        src: ../templates/filesystem.yaml
        dest: "{{ gpfsfolder }}/filesystem.yaml"

    - name: Apply the filesystem
      tags:
        - 7_gpfs
      ansible.builtin.shell: |
        set -ex
        export KUBECONFIG=./auth/kubeconfig
        {{ oc_bin }} apply -f "{{ gpfsfolder }}/filesystem.yaml"
      args:
        chdir: "{{ ocpfolder }}"

    - name: Template the snapshotclass
      tags:
        - 7_gpfs
      ansible.builtin.template:
        src: ../templates/snapshot.yaml
        dest: "{{ gpfsfolder }}/snapshot.yaml"

    - name: Apply the snapshotclass
      tags:
        - 7_gpfs
      ansible.builtin.shell: |
        set -ex
        export KUBECONFIG=./auth/kubeconfig
        {{ oc_bin }} apply -f "{{ gpfsfolder }}/snapshot.yaml"
      args:
        chdir: "{{ ocpfolder }}"

    - name: Template the test deployment
      tags:
        - 8_gpfs
      ansible.builtin.template:
        src: ../templates/test_consume.yaml
        dest: "{{ gpfsfolder }}/test_consume.yaml"

    - name: Apply the test deployment
      tags:
        - 8_gpfs
      ansible.builtin.shell: |
        set -ex
        export KUBECONFIG=./auth/kubeconfig
        {{ oc_bin }} apply -f "{{ gpfsfolder }}/test_consume.yaml"
      args:
        chdir: "{{ ocpfolder }}"

